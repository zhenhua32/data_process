{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tokenizers import BertWordPieceTokenizer, Tokenizer\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace, BertPreTokenizer\n",
    "import os\n",
    "import json\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForPreTraining,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizerFast,\n",
    "    BertConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这种方式构建的词更多些\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "vocab_size = 2000\n",
    "files = [\"ifeng_data/train.txt\", \"ifeng_data/test.txt\"]\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "trainer = WordPieceTrainer(vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "tokenizer.pre_tokenizer = BertPreTokenizer()\n",
    "tokenizer.train(files, trainer)\n",
    "model_path = \"pretrained_bert2\"\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "# TODO: 但是具体该怎么保存还有疑惑, BertTokenizerFast 无法加载\n",
    "# tokenizer.save(\"pretrained_bert2/tokenizer.json\")\n",
    "tokenizer.model.save(model_path)\n",
    "\n",
    "max_length = 64\n",
    "tokenizer.enable_truncation(max_length=max_length)\n",
    "# with open(os.path.join(model_path, \"vocab_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "#     tokenizer_config = {\n",
    "#         \"do_lower_case\": False,\n",
    "#         \"unk_token\": \"[UNK]\",\n",
    "#         \"sep_token\": \"[SEP]\",\n",
    "#         \"pad_token\": \"[PAD]\",\n",
    "#         \"cls_token\": \"[CLS]\",\n",
    "#         \"mask_token\": \"[MASK]\",\n",
    "#         \"model_max_length\": max_length,\n",
    "#         \"max_len\": max_length,\n",
    "#     }\n",
    "#     json.dump(tokenizer_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 2000\n",
    "files = [\"ifeng_data/train.txt\", \"ifeng_data/test.txt\"]\n",
    "max_length = 64\n",
    "truncate_longer_samples = True\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "\n",
    "# 构建并训练分词器, 使用这种方式构建的分词器可能不完整, 会缺失部分中文单字\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "tokenizer.train(\n",
    "    files=files,\n",
    "    vocab_size=vocab_size,\n",
    "    special_tokens=special_tokens,\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"pretrained_bert\"\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "\n",
    "tokenizer.save_model(model_path)\n",
    "with open(os.path.join(model_path, \"vocab_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    tokenizer_config = {\n",
    "        \"do_lower_case\": False,\n",
    "        \"unk_token\": \"[UNK]\",\n",
    "        \"sep_token\": \"[SEP]\",\n",
    "        \"pad_token\": \"[PAD]\",\n",
    "        \"cls_token\": \"[CLS]\",\n",
    "        \"mask_token\": \"[MASK]\",\n",
    "        \"model_max_length\": max_length,\n",
    "        \"max_len\": max_length,\n",
    "    }\n",
    "    json.dump(tokenizer_config, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新加载分词器, 使用快速版分词器会快很多\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"pretrained_bert2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5964"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 1456, 2765, 2782, 1379, 2326, 727, 519, 2939, 4, 1806, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"杨采钰明艳复古风[MASK]片\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_with_truncation(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def encode_without_truncation(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "# 加载训练数据集\n",
    "d = load_dataset(\"csv\", data_files={\"train\": \"ifeng_data/train.txt\", \"test\": \"ifeng_data/test.txt\"}, sep=\"\\t\", names=[\"text\"])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation\n",
    "# 使用完整数据集\n",
    "train_dataset = d[\"train\"].map(encode, batched=True)\n",
    "test_dataset = d[\"test\"].map(encode, batched=True)\n",
    "\n",
    "if truncate_longer_samples:\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "else:\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8025d30852334e6e768ca567da121c1aa274c2c5a5a8a9ff400eded44c1a99b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('transformers')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
