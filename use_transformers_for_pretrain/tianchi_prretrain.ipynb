{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "import os\n",
    "import json\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForPreTraining,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizerFast,\n",
    "    BertConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据来源是\n",
    "# https://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531810/train_set.csv.zip\n",
    "# https://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531810/test_a.csv.zip\n",
    "\n",
    "df_train = pd.read_csv(\"data/tianchi_news/train_set.csv\", sep=\"\\t\")\n",
    "df_test = pd.read_csv(\"data/tianchi_news/test_a.csv\", sep=\"\\t\")\n",
    "\n",
    "vocab = defaultdict(int)\n",
    "for line in df_train[\"text\"].to_list() + df_test[\"text\"].to_list():\n",
    "    for word in line.strip().split(\" \"):\n",
    "        vocab[word] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6977\n",
      "6984\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"]\n",
    "all_tokens = special_tokens + sorted(vocab.keys())\n",
    "print(len(all_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"text\"].to_csv(\"train.txt\", header=None, index=None)\n",
    "df_test[\"text\"].to_csv(\"test.txt\", header=None, index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(all_tokens)\n",
    "files = [\"train.txt\", \"test.txt\"]\n",
    "max_length = 1024\n",
    "truncate_longer_samples = True\n",
    "\n",
    "# 构建并训练分词器\n",
    "# tokenizer = BertWordPieceTokenizer()\n",
    "# tokenizer.train(\n",
    "#     files=files,\n",
    "#     vocab_size=vocab_size,\n",
    "#     special_tokens=special_tokens,\n",
    "# )\n",
    "# tokenizer.enable_truncation(max_length=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"pretrained_bert\"\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# 直接用这个也是可以的\n",
    "with open(os.path.join(model_path, \"vocab.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(all_tokens))\n",
    "\n",
    "# 决定不用这个了\n",
    "# tokenizer.save_model(model_path)\n",
    "with open(os.path.join(model_path, \"vocab_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    tokenizer_config = {\n",
    "        \"do_lower_case\": True,\n",
    "        \"unk_token\": \"[UNK]\",\n",
    "        \"sep_token\": \"[SEP]\",\n",
    "        \"pad_token\": \"[PAD]\",\n",
    "        \"cls_token\": \"[CLS]\",\n",
    "        \"mask_token\": \"[MASK]\",\n",
    "        \"model_max_length\": max_length,\n",
    "        \"max_len\": max_length,\n",
    "    }\n",
    "    json.dump(tokenizer_config, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新加载分词器, 使用快速版分词器会快很多\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_with_truncation(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def encode_without_truncation(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        return_special_tokens_mask=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d8fd779be73f89cd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\tzh\\.cache\\huggingface\\datasets\\csv\\default-d8fd779be73f89cd\\0.0.0\\6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 978.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\tzh\\.cache\\huggingface\\datasets\\csv\\default-d8fd779be73f89cd\\0.0.0\\6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 18.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 200000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载训练数据集\n",
    "d = load_dataset(\"csv\", data_files={\"train\": \"train.txt\", \"test\": \"test.txt\"}, sep=\"\\t\", names=[\"text\"])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.11s/ba]\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.18s/ba]\n"
     ]
    }
   ],
   "source": [
    "encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation\n",
    "# 使用完整数据集\n",
    "# train_dataset = d[\"train\"].map(encode, batched=True)\n",
    "# test_dataset = d[\"test\"].map(encode, batched=True)\n",
    "\n",
    "# 先使用小批量数据\n",
    "train_dataset = Dataset.from_dict(d[\"train\"][:10000]).map(encode, batched=True)\n",
    "test_dataset = Dataset.from_dict(d[\"test\"][:10000]).map(encode, batched=True)\n",
    "\n",
    "if truncate_longer_samples:\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "else:\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # 先变成单个字典, 值是一个巨大的数组\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # 后面这个是获取 keys 中的第一个 key, 然后取出 concatenated_examples[k]\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    # 然后将值变成数组的数组 每个小数组的长度是 max_length\n",
    "    result = {\n",
    "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)] for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "if not truncate_longer_samples:\n",
    "    train_dataset = train_dataset.map(group_texts, batched=True)\n",
    "    test_dataset = test_dataset.map(group_texts, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask, text. If special_tokens_mask, text are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "C:\\Anaconda3\\envs\\transformers\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 12500\n",
      "  0%|          | 8/12500 [00:21<8:21:54,  2.41s/it] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30060/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\envs\\transformers\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1400\u001b[0m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1402\u001b[1;33m                 if (\n\u001b[0m\u001b[0;32m   1403\u001b[0m                     \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1404\u001b[0m                     \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21ade77d1a005a9febcfd29d2ec0dea48f74b0b2dc097f0a2ba8132806873fea"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('transformers')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
