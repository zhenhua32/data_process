{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2967 6758 339 2021 1854 3731 4109 3792 4149 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>4464 486 6352 5619 2465 4802 1452 3137 5778 54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7346 4068 5074 3747 5681 6093 1777 2226 7354 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>7159 948 4866 2109 5520 2490 211 3956 5520 549...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3646 3055 3055 2490 4659 6065 3370 5814 2465 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>2</td>\n",
       "      <td>307 4894 7539 4853 5330 648 6038 4409 3764 603...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>2</td>\n",
       "      <td>3792 2983 355 1070 4464 5050 6298 3782 3130 68...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>11</td>\n",
       "      <td>6811 1580 7539 1252 1899 5139 1386 3870 4124 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>2</td>\n",
       "      <td>6405 3203 6644 983 794 1913 1678 5736 1397 191...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>3</td>\n",
       "      <td>4350 3878 3268 1699 6909 5505 2376 2465 6088 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                               text\n",
       "0           2  2967 6758 339 2021 1854 3731 4109 3792 4149 15...\n",
       "1          11  4464 486 6352 5619 2465 4802 1452 3137 5778 54...\n",
       "2           3  7346 4068 5074 3747 5681 6093 1777 2226 7354 6...\n",
       "3           2  7159 948 4866 2109 5520 2490 211 3956 5520 549...\n",
       "4           3  3646 3055 3055 2490 4659 6065 3370 5814 2465 5...\n",
       "...       ...                                                ...\n",
       "199995      2  307 4894 7539 4853 5330 648 6038 4409 3764 603...\n",
       "199996      2  3792 2983 355 1070 4464 5050 6298 3782 3130 68...\n",
       "199997     11  6811 1580 7539 1252 1899 5139 1386 3870 4124 1...\n",
       "199998      2  6405 3203 6644 983 794 1913 1678 5736 1397 191...\n",
       "199999      3  4350 3878 3268 1699 6909 5505 2376 2465 6088 2...\n",
       "\n",
       "[200000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/train_set.csv\", sep=\"\\t\", header=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 160000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 40000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df).shuffle(seed=42)\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512\n",
    "def map_func(examples):\n",
    "    # examples 还是个 dict\n",
    "    result = {\"input_ids\": [], \"token_type_ids\": [], \"attention_mask\": []}\n",
    "    for text in examples[\"text\"]:\n",
    "        input_ids = [tokenizer.cls_token_id] + [int(x) for x in text.strip().split(\" \")][:max_len-2] + [tokenizer.sep_token_id]\n",
    "        input_len = len(input_ids)\n",
    "        token_type_ids = [0] * max_len\n",
    "        attention_mask = [1] * input_len + [0] * (max_len - input_len)\n",
    "        input_ids = input_ids + [0] * (max_len - input_len)\n",
    "\n",
    "        result[\"input_ids\"].append(input_ids)\n",
    "        result[\"token_type_ids\"].append(token_type_ids)\n",
    "        result[\"attention_mask\"].append(attention_mask)\n",
    "    if \"label\" in examples:\n",
    "        result[\"label\"] = examples[\"label\"]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:24<00:00,  6.45ba/s]\n",
      "100%|██████████| 40/40 [00:06<00:00,  6.41ba/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset[\"train\"].map(map_func, batched=True)\n",
    "eval_dataset = dataset[\"test\"].map(map_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "model_path = \"tianchi_model\"\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=128,\n",
    "    logging_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=10,\n",
    "    load_best_model_at_end=True,\n",
    "    warmup_steps=3000,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate = 1e-4,\n",
    "    report_to=\"tensorboard\",\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\zhenh\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\accuracy\\3e9ee15abf476145152fe4e9a9c1463ff95d3d65cdc555be9cfe061bdaeb1a14 (last modified on Sat Jan 15 11:26:02 2022) since it couldn't be found locally at accuracy, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\t\t# 需要一个元组, 第一个是 logits, 第二个是标签\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running training *****\n",
      "  Num examples = 160000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 12500\n",
      "  4%|▍         | 500/12500 [07:27<2:58:49,  1.12it/s]The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7407, 'learning_rate': 1.6633333333333336e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "  4%|▍         | 500/12500 [10:32<2:58:49,  1.12it/s]Saving model checkpoint to tianchi_model\\checkpoint-500\n",
      "Configuration saved in tianchi_model\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.916534960269928, 'eval_accuracy': 0.7331, 'eval_runtime': 184.1196, 'eval_samples_per_second': 217.25, 'eval_steps_per_second': 1.7, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-500\\pytorch_model.bin\n",
      "  8%|▊         | 1000/12500 [17:58<2:50:37,  1.12it/s] The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6206, 'learning_rate': 3.33e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      "  8%|▊         | 1000/12500 [21:01<2:50:37,  1.12it/s]Saving model checkpoint to tianchi_model\\checkpoint-1000\n",
      "Configuration saved in tianchi_model\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.39181092381477356, 'eval_accuracy': 0.891525, 'eval_runtime': 182.3899, 'eval_samples_per_second': 219.31, 'eval_steps_per_second': 1.716, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-1000\\pytorch_model.bin\n",
      " 12%|█▏        | 1500/12500 [28:29<2:43:23,  1.12it/s]  The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3926, 'learning_rate': 4.996666666666667e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 12%|█▏        | 1500/12500 [31:32<2:43:23,  1.12it/s]Saving model checkpoint to tianchi_model\\checkpoint-1500\n",
      "Configuration saved in tianchi_model\\checkpoint-1500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3380712568759918, 'eval_accuracy': 0.9005, 'eval_runtime': 182.4041, 'eval_samples_per_second': 219.293, 'eval_steps_per_second': 1.716, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-1500\\pytorch_model.bin\n",
      " 16%|█▌        | 2000/12500 [38:56<2:35:26,  1.13it/s]  The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3185, 'learning_rate': 6.663333333333333e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 16%|█▌        | 2000/12500 [42:00<2:35:26,  1.13it/s]Saving model checkpoint to tianchi_model\\checkpoint-2000\n",
      "Configuration saved in tianchi_model\\checkpoint-2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27666887640953064, 'eval_accuracy': 0.92095, 'eval_runtime': 183.6094, 'eval_samples_per_second': 217.854, 'eval_steps_per_second': 1.705, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-2000\\pytorch_model.bin\n",
      " 20%|██        | 2500/12500 [49:27<2:28:13,  1.12it/s]  The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2889, 'learning_rate': 8.326666666666667e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 20%|██        | 2500/12500 [52:30<2:28:13,  1.12it/s]Saving model checkpoint to tianchi_model\\checkpoint-2500\n",
      "Configuration saved in tianchi_model\\checkpoint-2500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25703468918800354, 'eval_accuracy': 0.927425, 'eval_runtime': 182.361, 'eval_samples_per_second': 219.345, 'eval_steps_per_second': 1.716, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-2500\\pytorch_model.bin\n",
      " 24%|██▍       | 3000/12500 [59:57<2:21:32,  1.12it/s]  The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2507, 'learning_rate': 9.99e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 24%|██▍       | 3000/12500 [1:03:02<2:21:32,  1.12it/s]Saving model checkpoint to tianchi_model\\checkpoint-3000\n",
      "Configuration saved in tianchi_model\\checkpoint-3000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24435913562774658, 'eval_accuracy': 0.9302, 'eval_runtime': 184.5538, 'eval_samples_per_second': 216.739, 'eval_steps_per_second': 1.696, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-3000\\pytorch_model.bin\n",
      " 28%|██▊       | 3500/12500 [1:10:44<2:22:45,  1.05it/s]  The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2523, 'learning_rate': 9.476842105263159e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 28%|██▊       | 3500/12500 [1:13:59<2:22:45,  1.05it/s]Saving model checkpoint to tianchi_model\\checkpoint-3500\n",
      "Configuration saved in tianchi_model\\checkpoint-3500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23677566647529602, 'eval_accuracy': 0.931525, 'eval_runtime': 194.6549, 'eval_samples_per_second': 205.492, 'eval_steps_per_second': 1.608, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-3500\\pytorch_model.bin\n",
      " 32%|███▏      | 4000/12500 [1:21:32<2:07:03,  1.11it/s]  The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2239, 'learning_rate': 8.950526315789473e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 32%|███▏      | 4000/12500 [1:24:46<2:07:03,  1.11it/s]Saving model checkpoint to tianchi_model\\checkpoint-4000\n",
      "Configuration saved in tianchi_model\\checkpoint-4000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2183522880077362, 'eval_accuracy': 0.934625, 'eval_runtime': 194.2302, 'eval_samples_per_second': 205.941, 'eval_steps_per_second': 1.611, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-4000\\pytorch_model.bin\n",
      " 36%|███▌      | 4500/12500 [1:32:19<1:56:30,  1.14it/s]  The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2141, 'learning_rate': 8.42421052631579e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 36%|███▌      | 4500/12500 [1:35:35<1:56:30,  1.14it/s]Saving model checkpoint to tianchi_model\\checkpoint-4500\n",
      "Configuration saved in tianchi_model\\checkpoint-4500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19585053622722626, 'eval_accuracy': 0.942525, 'eval_runtime': 195.5031, 'eval_samples_per_second': 204.6, 'eval_steps_per_second': 1.601, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-4500\\pytorch_model.bin\n",
      " 40%|████      | 5000/12500 [1:43:05<1:53:51,  1.10it/s]  The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2002, 'learning_rate': 7.897894736842106e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 40%|████      | 5000/12500 [1:46:27<1:53:51,  1.10it/s]Saving model checkpoint to tianchi_model\\checkpoint-5000\n",
      "Configuration saved in tianchi_model\\checkpoint-5000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19505470991134644, 'eval_accuracy': 0.943825, 'eval_runtime': 201.5553, 'eval_samples_per_second': 198.457, 'eval_steps_per_second': 1.553, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-5000\\pytorch_model.bin\n",
      " 44%|████▍     | 5500/12500 [1:54:14<1:48:48,  1.07it/s]  The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1578, 'learning_rate': 7.371578947368422e-05, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 44%|████▍     | 5500/12500 [1:57:32<1:48:48,  1.07it/s]Saving model checkpoint to tianchi_model\\checkpoint-5500\n",
      "Configuration saved in tianchi_model\\checkpoint-5500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20414797961711884, 'eval_accuracy': 0.944, 'eval_runtime': 198.2002, 'eval_samples_per_second': 201.816, 'eval_steps_per_second': 1.579, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-5500\\pytorch_model.bin\n",
      "Deleting older checkpoint [tianchi_model\\checkpoint-500] due to args.save_total_limit\n",
      " 48%|████▊     | 6000/12500 [2:05:23<1:43:37,  1.05it/s]  The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1636, 'learning_rate': 6.845263157894737e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 48%|████▊     | 6000/12500 [2:08:43<1:43:37,  1.05it/s]Saving model checkpoint to tianchi_model\\checkpoint-6000\n",
      "Configuration saved in tianchi_model\\checkpoint-6000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1876252144575119, 'eval_accuracy': 0.946025, 'eval_runtime': 199.8779, 'eval_samples_per_second': 200.122, 'eval_steps_per_second': 1.566, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-6000\\pytorch_model.bin\n",
      "Deleting older checkpoint [tianchi_model\\checkpoint-1000] due to args.save_total_limit\n",
      " 52%|█████▏    | 6500/12500 [2:16:31<1:34:29,  1.06it/s]  The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1433, 'learning_rate': 6.318947368421053e-05, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 52%|█████▏    | 6500/12500 [2:19:52<1:34:29,  1.06it/s]Saving model checkpoint to tianchi_model\\checkpoint-6500\n",
      "Configuration saved in tianchi_model\\checkpoint-6500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1968160718679428, 'eval_accuracy': 0.9484, 'eval_runtime': 201.3272, 'eval_samples_per_second': 198.682, 'eval_steps_per_second': 1.555, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-6500\\pytorch_model.bin\n",
      "Deleting older checkpoint [tianchi_model\\checkpoint-1500] due to args.save_total_limit\n",
      " 56%|█████▌    | 7000/12500 [2:27:39<1:24:29,  1.09it/s]  The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1443, 'learning_rate': 5.7926315789473686e-05, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 56%|█████▌    | 7000/12500 [2:30:57<1:24:29,  1.09it/s]Saving model checkpoint to tianchi_model\\checkpoint-7000\n",
      "Configuration saved in tianchi_model\\checkpoint-7000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1822524219751358, 'eval_accuracy': 0.949225, 'eval_runtime': 198.2574, 'eval_samples_per_second': 201.758, 'eval_steps_per_second': 1.579, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-7000\\pytorch_model.bin\n",
      "Deleting older checkpoint [tianchi_model\\checkpoint-2000] due to args.save_total_limit\n",
      " 60%|██████    | 7500/12500 [2:38:44<1:18:59,  1.05it/s] The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1423, 'learning_rate': 5.266315789473685e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 60%|██████    | 7500/12500 [2:42:05<1:18:59,  1.05it/s]Saving model checkpoint to tianchi_model\\checkpoint-7500\n",
      "Configuration saved in tianchi_model\\checkpoint-7500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1638089120388031, 'eval_accuracy': 0.953825, 'eval_runtime': 200.477, 'eval_samples_per_second': 199.524, 'eval_steps_per_second': 1.561, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-7500\\pytorch_model.bin\n",
      "Deleting older checkpoint [tianchi_model\\checkpoint-2500] due to args.save_total_limit\n",
      " 64%|██████▍   | 8000/12500 [2:49:51<1:11:10,  1.05it/s] The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1003, 'learning_rate': 4.74e-05, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 64%|██████▍   | 8000/12500 [2:53:12<1:11:10,  1.05it/s]Saving model checkpoint to tianchi_model\\checkpoint-8000\n",
      "Configuration saved in tianchi_model\\checkpoint-8000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17067012190818787, 'eval_accuracy': 0.953325, 'eval_runtime': 200.9415, 'eval_samples_per_second': 199.063, 'eval_steps_per_second': 1.558, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-8000\\pytorch_model.bin\n",
      "Deleting older checkpoint [tianchi_model\\checkpoint-3000] due to args.save_total_limit\n",
      " 68%|██████▊   | 8500/12500 [3:00:57<1:03:52,  1.04it/s] The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0942, 'learning_rate': 4.214736842105263e-05, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 68%|██████▊   | 8500/12500 [3:04:13<1:03:52,  1.04it/s]Saving model checkpoint to tianchi_model\\checkpoint-8500\n",
      "Configuration saved in tianchi_model\\checkpoint-8500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1975991278886795, 'eval_accuracy': 0.9512, 'eval_runtime': 196.386, 'eval_samples_per_second': 203.681, 'eval_steps_per_second': 1.594, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-8500\\pytorch_model.bin\n",
      "Deleting older checkpoint [tianchi_model\\checkpoint-3500] due to args.save_total_limit\n",
      " 72%|███████▏  | 9000/12500 [3:11:58<53:22,  1.09it/s]   The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0945, 'learning_rate': 3.6884210526315796e-05, 'epoch': 3.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 72%|███████▏  | 9000/12500 [3:15:17<53:22,  1.09it/s]Saving model checkpoint to tianchi_model\\checkpoint-9000\n",
      "Configuration saved in tianchi_model\\checkpoint-9000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17557576298713684, 'eval_accuracy': 0.9531, 'eval_runtime': 199.7076, 'eval_samples_per_second': 200.293, 'eval_steps_per_second': 1.567, 'epoch': 3.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-9000\\pytorch_model.bin\n",
      "Deleting older checkpoint [tianchi_model\\checkpoint-4000] due to args.save_total_limit\n",
      " 76%|███████▌  | 9500/12500 [3:23:05<46:27,  1.08it/s]   The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0937, 'learning_rate': 3.16421052631579e-05, 'epoch': 3.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \n",
      " 76%|███████▌  | 9500/12500 [3:26:25<46:27,  1.08it/s]Saving model checkpoint to tianchi_model\\checkpoint-9500\n",
      "Configuration saved in tianchi_model\\checkpoint-9500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.162276029586792, 'eval_accuracy': 0.9556, 'eval_runtime': 200.2144, 'eval_samples_per_second': 199.786, 'eval_steps_per_second': 1.563, 'epoch': 3.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-9500\\pytorch_model.bin\n",
      "Deleting older checkpoint [tianchi_model\\checkpoint-4500] due to args.save_total_limit\n",
      " 80%|████████  | 10000/12500 [3:34:11<38:52,  1.07it/s]  The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0883, 'learning_rate': 2.6378947368421054e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 80%|████████  | 10000/12500 [3:37:32<38:52,  1.07it/s]Saving model checkpoint to tianchi_model\\checkpoint-10000\n",
      "Configuration saved in tianchi_model\\checkpoint-10000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.165143221616745, 'eval_accuracy': 0.956525, 'eval_runtime': 201.6139, 'eval_samples_per_second': 198.399, 'eval_steps_per_second': 1.552, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-10000\\pytorch_model.bin\n",
      "Deleting older checkpoint [tianchi_model\\checkpoint-5000] due to args.save_total_limit\n",
      " 84%|████████▍ | 10500/12500 [3:45:22<32:07,  1.04it/s]   The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0537, 'learning_rate': 2.111578947368421e-05, 'epoch': 4.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 84%|████████▍ | 10500/12500 [3:48:43<32:07,  1.04it/s]Saving model checkpoint to tianchi_model\\checkpoint-10500\n",
      "Configuration saved in tianchi_model\\checkpoint-10500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1821676790714264, 'eval_accuracy': 0.956475, 'eval_runtime': 201.0259, 'eval_samples_per_second': 198.979, 'eval_steps_per_second': 1.557, 'epoch': 4.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-10500\\pytorch_model.bin\n",
      "Deleting older checkpoint [tianchi_model\\checkpoint-5500] due to args.save_total_limit\n",
      " 88%|████████▊ | 11000/12500 [3:56:40<23:33,  1.06it/s]   The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0555, 'learning_rate': 1.5852631578947368e-05, 'epoch': 4.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 88%|████████▊ | 11000/12500 [4:00:04<23:33,  1.06it/s]Saving model checkpoint to tianchi_model\\checkpoint-11000\n",
      "Configuration saved in tianchi_model\\checkpoint-11000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17478419840335846, 'eval_accuracy': 0.95655, 'eval_runtime': 203.7552, 'eval_samples_per_second': 196.314, 'eval_steps_per_second': 1.536, 'epoch': 4.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-11000\\pytorch_model.bin\n",
      "Deleting older checkpoint [tianchi_model\\checkpoint-6000] due to args.save_total_limit\n",
      " 92%|█████████▏| 11500/12500 [4:07:51<15:10,  1.10it/s]   The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0477, 'learning_rate': 1.0589473684210526e-05, 'epoch': 4.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 92%|█████████▏| 11500/12500 [4:11:12<15:10,  1.10it/s]Saving model checkpoint to tianchi_model\\checkpoint-11500\n",
      "Configuration saved in tianchi_model\\checkpoint-11500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1879570037126541, 'eval_accuracy': 0.95715, 'eval_runtime': 200.9585, 'eval_samples_per_second': 199.046, 'eval_steps_per_second': 1.558, 'epoch': 4.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-11500\\pytorch_model.bin\n",
      "Deleting older checkpoint [tianchi_model\\checkpoint-6500] due to args.save_total_limit\n",
      " 96%|█████████▌| 12000/12500 [4:18:34<06:58,  1.19it/s]   The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0497, 'learning_rate': 5.326315789473684e-06, 'epoch': 4.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 96%|█████████▌| 12000/12500 [4:21:36<06:58,  1.19it/s]Saving model checkpoint to tianchi_model\\checkpoint-12000\n",
      "Configuration saved in tianchi_model\\checkpoint-12000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17882849276065826, 'eval_accuracy': 0.957825, 'eval_runtime': 181.5579, 'eval_samples_per_second': 220.315, 'eval_steps_per_second': 1.724, 'epoch': 4.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-12000\\pytorch_model.bin\n",
      "Deleting older checkpoint [tianchi_model\\checkpoint-7000] due to args.save_total_limit\n",
      "100%|██████████| 12500/12500 [4:28:40<00:00,  1.19it/s]  The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40000\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0461, 'learning_rate': 6.315789473684211e-08, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      "100%|██████████| 12500/12500 [4:31:41<00:00,  1.19it/s]Saving model checkpoint to tianchi_model\\checkpoint-12500\n",
      "Configuration saved in tianchi_model\\checkpoint-12500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18184685707092285, 'eval_accuracy': 0.957775, 'eval_runtime': 181.2177, 'eval_samples_per_second': 220.729, 'eval_steps_per_second': 1.727, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in tianchi_model\\checkpoint-12500\\pytorch_model.bin\n",
      "Deleting older checkpoint [tianchi_model\\checkpoint-7500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from tianchi_model\\checkpoint-9500 (score: 0.162276029586792).\n",
      "100%|██████████| 12500/12500 [4:31:43<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 16303.3632, 'train_samples_per_second': 49.07, 'train_steps_per_second': 0.767, 'train_loss': 0.23910002365112304, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12500, training_loss=0.23910002365112304, metrics={'train_runtime': 16303.3632, 'train_samples_per_second': 49.07, 'train_steps_per_second': 0.767, 'train_loss': 0.23910002365112304, 'epoch': 5.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in tianchi_model\\config.json\n",
      "Model weights saved in tianchi_model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:07<00:00,  6.36ba/s]\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "test_df = pd.read_csv(\"data/test_a.csv\", sep=\"\\t\", header=0)\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_df).map(map_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 128\n",
      "1173it [28:24,  1.78it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[ 0.7617 ,  8.65   , -2.686  , ..., -1.881  , -3.012  , -3.59   ],\n",
       "       [-1.743  , -1.503  , 11.46   , ..., -0.545  ,  2.201  , -0.7485 ],\n",
       "       [-0.6426 ,  0.3857 , -1.32   , ..., -0.10345, -2.34   , -0.8706 ],\n",
       "       ...,\n",
       "       [-0.678  ,  7.69   , -2.492  , ..., -2.531  , -1.758  , -1.978  ],\n",
       "       [-0.1014 , -0.647  ,  0.5474 , ...,  1.251  , -2.979  , -2.889  ],\n",
       "       [ 0.2166 ,  9.3    , -2.232  , ..., -1.683  , -2.373  , -3.08   ]],\n",
       "      dtype=float16), label_ids=None, metrics={'test_runtime': 245.8173, 'test_samples_per_second': 203.403, 'test_steps_per_second': 1.591})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_result = trainer.predict(test_dataset)\n",
    "predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "0          1\n",
       "1          2\n",
       "2          8\n",
       "3          5\n",
       "4          0\n",
       "...      ...\n",
       "49995      0\n",
       "49996     13\n",
       "49997      1\n",
       "49998      3\n",
       "49999      1\n",
       "\n",
       "[50000 rows x 1 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df = pd.DataFrame(np.argmax(predict_result.predictions, axis=-1).tolist(), columns=[\"label\"])\n",
    "predict_df.to_csv(\"predict.csv\", index=False)\n",
    "predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 102, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"hello\", padding=\"max_length\", truncation=True, max_length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 12, 24, 102, 0, 0, 0, 0, 0, 0],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_func({\"text\": \"12 24\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.cls_token_id)\n",
    "print(tokenizer.sep_token_id)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7c70413a5810a6397efd487546398fda6db6fc006e875c25a748979a67c1938"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
